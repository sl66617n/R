---
title: "Midterm"
author: "Sijia Liang"
date: "3/15/2018"
output:
  pdf_document:
      latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 2.2
Write a program in R that is a function of a positive integer j and produces a vector of the form
$$1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, ... , 1, 2, ..., j.$$
```{r Exercise 2.2}
series <- function(j) {
  result <- c()
  for (i in 1:j) {
    result <- c(result, seq(i))
  }
  return(result)
}
series(10)

```

## Exercise 2.5
Look at the academic score data in Table 1.2. The various scores that went into this table have been standardized across countries to adjust for cultural differences.

**(a)** Are the standard deviation of the different scores comparable? Consider using the `bartlett.test()` test for equality of variances. Other tests of equality of variances available in R are `var.test()`, `flinger.test()`, `ansari.test()`, and `mood.test()`. Using the `help()` file to read about these tests. 
```{r Exercise 2.5 a}
OECD <- read.table("/Users/sijialiang/Desktop/Supp_2/OECD PISA.txt", header=TRUE,row.names=1)
sapply(OECD, sd) # compute sd for every column, yes, they are comparable

bartlett.test(OECD)

help("var.test")
help("ansari.test")
help("mood.test")

```

**(b)** Examine the correlation matrix of this data using the `cor` function. Describe what you see. See also Exercise 8.7.

```{r Exercise 2.5 b}

cor(OECD)
# All of the test scores are very highly correlated with each other. 
# This suggest any one test score is representative of the whole data from each country.
```

## Exercise 2.8
Consider the function

$$f(x)=\dfrac{1}{2+\sin(5\pi x)}$$
for values of $$0 \leqslant x \leqslant1.$$
**(a)** Plot the function f in R. (See Sect.3.1 for an example of how to do this.)

```{r Exercise 2.8 a}
x <- seq(0, 1, by=0.05)
f <- function(x) {
  return(1/(2+sin(5*pi*x)))
}

y <- f(x)
plot(x, y, type="l")
```
**(b)** Find the area under this curve using numerical quadrature with `integrate()`.
```{r Exercise 2.8 b}
integrate(f, 0, 1)
```

**(c)** Identify maximums and minimums of this function in R using `nlm()`. Show how the results depend on the starting values.
```{r Exercise 2.8 c}

#  minimum value
nlm(f, 0.9)$minimum


g <- function(x) {
  return(0-f(x))
}
#maximum value
-nlm(g, 0.2)$minimum

nlm(g, 0.1)
nlm(g, 0.2)
nlm(g, 0.9)
# indenpent on starting value since the local max are all the same

```

## Exercise 2.12

What does R do when we try to access subscripts that are out of range? Suppose we start with `> x <- 1:3`
**(a)** A negative subscript such as `x[-2]` will omit the second element of the vector. What does `x[-5]` yield in this example?
```{r Exercise 2.12 a}
# it does not show elements that are out of range

x <- 1:3
# omit the second element of the vector
x[-2]

# it remains the same on apperance, but it actually omit the 5th element of the vector
x[-5]

```
**(b)** What does `x[0]` give us? Can you explain this outcome?
```{r Exercise 2.12 b}
x[0]
# gives zero, because R starts with 1, unlike Java, C++ start with zero
```
**(c)** Suppose we try to assign a value to an invalid element of the vector, such as in `x[7] <- 9`. What does this produce?
```{r Exercise 2.12 c}
x[7] <- 9
x
# it assigned the value 9 to the 7th element of the vector
# but at the same time, returns NotAvailable(NA) for 4,5,6th element
```
**(d)** Is an empty subscript `x[]` different from `x` with no subscript at all? As an example, how is this `x[] <- 3` different from `x <- 3`?
```{r Exercise 2.12 d}
x[]
x # its the same

x[] <- 3
x # assign 3 to the vector
x <- 3
x # assign the value 3 to x
```

## Exercise 5.1

Consider some data such as the apartment rent values or the CD4 counts. Transform these values to more normal-looking distributions using the Box-Cox transformation (5.5) for different values of \(\lambda \). Find the value of \(\lambda \) that provides the best fit according to the Jarque-bera test, the Kolmogorov-Smirnov test, or the Shapiro-Wilk test. Are these different \(\lambda \)s close in value? Explain why this may (or not) be the case.
```{r Exercise 5.1}
# Different lambdas are similar
# because all tests are testing whether data are normally distributed
# we try out all different tests and lambda values in the following
# to find the best fit

# Loading packages
#install.packages("fBasics")
library(fBasics)
#install.packages("akima")
library(akima)


# Loading housing dataset and plot
housing <- read.table("/Users/sijialiang/Desktop/Supp_2/housing.txt", header=TRUE,row.names=1)
housing$Apartment
hist(housing$Apartment)
log(housing$Apartment)
hist(log(housing$Apartment))

# Loading CD4 dataset
require(boot)
cd4
# stem and leaf plot for small dataset
hist(cd4$baseline, breaks=20)
stem(cd4$baseline)


# assuming statistical signifiance = 5%, 0.05

# jb Test for housing$Apartment
jbTest(housing$Apartment,
       title="Original apartment rents")
# p value = 0.043 reject null hypotheis, housing$Apartment is not normal distributed, 
# therefore, perform box-cox transformation

# jb Test for cd4
jbTest(cd4$baseline)
# p value 0.79 can't reject null hypothesis


# One-sample Kolmgorov-Smirnov test for housing$Apartment
z1 <- housing$Apartment
z1 <- (z1-mean(z1)) / sd(z1)
ksnormTest(z1)# two-sided = 0.1484 =p-value > 0.05 = can't reject null hypothesis

#### this is the best since it has the largest p value among all other tests

z2 <- -(housing$Apartment)^(-1/2)
z2 <- (z2-mean(z2)) / sd(z2)
ksnormTest(z2) # two-sided = 0.4315 > 0.05 = can't reject

z3 <- log(housing$Apartment)
z3 <- (z3-mean(z3)) / sd(z3)
ksnormTest(z3)# two-sided =  0.2976> 0.05 = can't reject

z4 <- (housing$Apartment)^(1/2)
z4 <- (z4-mean(z4)) / sd(z4)
ksnormTest(z4)# two-sided = 0.2068 > 0.05 = can't reject

# One-sample Kolmgorov-Smirnov test for cd4
cdks <- cd4$baseline
cdks <- (cdks-mean(cdks)) / sd(cdks)
ksnormTest(cdks)# two-sided=0.9884> 0.05 = not reject null hypothesis

##### cd4 data, 0.9884 best lambda among others




# Shapiro test for housing$Apartment
shapiro.test(housing$Apartment) #0.007472 < 0.05 = reject, not normal, cox-box transformation
shapiro.test(-housing$Apartment^(-1/2)) # lambda=-1/2, p=0.3136>0.05, (-1/2) is better than take log
shapiro.test(log(housing$Apartment)) # when lambda=0, take log, p= 0.1481 >0.05, obey normal
shapiro.test((housing$Apartment)^(1/2)) # when lambda=1/2 >0, p=0.0414 < 0.05, failed

# Shapiro test for cd4$baseline
shapiro.test(cd4$baseline) # p=0.9434 >0.05

```

## Question 6

Let X have covariance matrix $$	\sum =\left(\begin{matrix}
25 & -2 & 4\\
-2 & 4 & 1\\
4 & 1 & 9\\
\end{matrix}\right)$$

**(a)** Determine R (correlation matrix) and (diagonal matrix with standard deviations on the diagonals)
```{r Exercise 6 a}
V_half <- diag(c(5,2,3),3,3)
R <- matrix(c(1,-1/5.0,4/15.,-1/5.,1,1/6.,4/15., 1/6.,1 ), nrow=3)
R
```
**(b)** Multiply your matrices to check the relation 
```{r Exercise 6 b}
V_half %*% R %*% V_half
```

## Question 7

Using the vectors  and ,verify the extended Cauchy-Schwarz inequality if $$B=\left(\begin{matrix}
2 & -2\\
-2 & 5\\
\end{matrix}\right)$$

```{r Exercise 7}
b <- c(-4,3)
b
d <- c(1,1)
d
B <- matrix(c(2,-2,-2,5), nrow=2)
B
b%*%d
b %*% B %*% b
d %*% solve(B) %*% d
(b%*%d)^2 <= (b %*% B %*% b)*(d %*% solve(B) %*% d)

```

## Question 8

The Table data show the age (x1, in years) and selling prices (x2, in thousands of dollars) of 10 used cars.

**(a)** Calculate the squared statistical distances.
```{r Exercise 8 a}
X <- matrix(c(1,2,3,3,4,5,6,8,9,10,18.95,19.00,17.95,15.54,14.00,12.95,8.94,7.49,6.00,3.99), nrow=10)
Sigma <- cov(X)
Sigma
X_bar <- colMeans(X)
X_bar
X_demean <- X - matrix(rep(X_bar,each=10),nrow=10)
X_demean
diag(X_demean %*% solve(Sigma) %*% t(X_demean))
qchisq(.5, df=2)
```


**(b)** Using the distances in part a, determine the proportion of the observations falling within the estimated 50% probability contour of a bivariate normal distribution.
```{r Exercise 8 b}
# TRUEs are the data falling in the 50% prob contour
diag(X_demean %*% solve(Sigma) %*% t(X_demean)) <= qchisq(.5, df=2)

```


