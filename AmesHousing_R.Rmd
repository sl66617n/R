---
title: "Ames Housing Project"
author: 'Sijia Liang '
date: "3/27/2018"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
---
##1 Introduction

This AmesHousing dataset contains about 80 explanatory variables to describe almost every aspect of the house. This dataset describes 2340 residential property sales in Ames, Iowa between 2006 and 2010.

Our goal is to build a predictive model on sale prices of houses using multiple linear regression. Objective of this project is to identify the most important features and best regression model.Less than 15 attributes were used as input in the best fitted model.

##2 Exploratory Data Analysis

We discovered that there are some numerical features such as following:

1. Square footage (1stFlrSF, GarageArea)

2. Time related features (when the home was built or sold)

3. Room and amenties data (bathrooms)

4. Condition and quality: (rated from 1–10)


Some categorical features such as following:

1. Neighborhood

2. External quality

3. Zoning





```{r, warning=FALSE,message=FALSE}
# load all packages
library(knitr)
library(ggplot2)
library(plyr)
library(dplyr)
library(corrplot)
library(caret)
library(gridExtra)
library(scales)
library(Rmisc)
library(ggrepel)
library(psych)
library(gdata)
library(tidyverse)
library(stringr)
library(lubridate)
library(graphics)
library(randomForest)
```


\newpage
```{r load data}
# read data using read.csv
train = read.csv(file="/Users/sijialiang/Desktop/Eclipse/MultiVarStats/AmesHousing_Training.csv")

```


Lets first take a look at a correlation plot on all numerical variables.
```{r correlationPlotone, fig1, fig.height = 4, fig.width = 6}
# create a variable named numVars that contains all numerical variables in the dateset
numVars <- which(sapply(train, is.numeric))

# assign the value to numVarNames for further use
numVarNames <- names(numVars)

# print out the numbers of numerical variables
cat('There exist', length(numVars), 'numeric variables in the Ames housing dataset.')

train_numVars <- train[, numVars]

# correlations of all numeric variables
cor_numVars <- cor(train_numVars, use="pairwise.complete.obs") 

# set it as a decreasing order on correlations with our y
cor_sorted <- as.matrix(sort(cor_numVars[,'SalePrice'], decreasing = TRUE))

# show only high correlations and assign the value to cor_high for use of furture plot
cor_high <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVars <- cor_numVars[cor_high, cor_high]

# plot using corrplot.mixed() function from 'corrplot' package
corrplot.mixed(cor_numVars, lower = "square", upper = "circle", tl.col="#990000", tl.pos = "lt")

```

\newpage
From the correlation plot, we determined that predictors like Overall quality, Ground living area, Total basement square feet and etc have the highest correlation with Sale Price, which make sense.

For multicollinearity, we determined that both GarageCars and GarageArea are highly correlated,same for GrLivArea and TotRmsAbvGrd, and YearBuilt and GarageYrBlt. Due to the reason that these variables are highly correlated and share the similar traits of the housing prices, therefore we decided to use only one of them in later regression model.





## 3 Data Preprocessing and Random Forest
```{r randomForest, fig1, fig.height = 3, fig.width = 5}
# find numeric columns
train_numeric=select_if(train, is.numeric)

# find NA in columns
#kable(sort(colSums(is.na(train_numeric)),decreasing = T))

# replace NA with 0
train_numeric[is.na(train_numeric)] <- 0

# find character columns
train_char=select_if(train, is.character)

# find NA in columns
#kable(sort(colSums(is.na(train_char)),decreasing = T))

# replace NA with 0
train_char[is.na(train_char)] <- 0

# convert to factor
train_char[] <- lapply(train_char, factor)

# combine numerical and char
train_new=cbind(train_numeric,train_char)

# take a look
#kable(head(train_new), caption = "Training Data Set")

# set random seed, lets say this year
set.seed(2018)

# quick and dirty randomForest model with 88 small number of trees
quick_randomForest <- randomForest(x=train_new[1:2340,-79], y=train_new$SalePrice[1:2340], ntree=88,importance=TRUE)

# set importance on the quick randomForest
importance_randomForest <- importance(quick_randomForest)
importance_DF <- data.frame(Variables = row.names(importance_randomForest), MSE = importance_randomForest[,1])

# set in decending order display
importance_DF <- importance_DF[order(importance_DF$MSE, decreasing = TRUE),]

# plot important variables using ggplot from 'ggplot2' package
ggplot(importance_DF[1:20,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position="none")

```


Clearly Overall.Qual, Year.Built, Gr.Liv.Area, X1st.Flr.SF, Garage.Area, Year.Remod.Add, should be picked as variables. Then we randomly select the rest of categorical and numerical variables on our preference such as MS.SubClass, Lot.Area.



## 4 Feature Selection and Visualization


Based on the correlation plot, randomForest and some common sense, we decided the following subset of the variables as predicators:

* Overall Cond - Overall condition rating

* Year Built - Original construction date

* X1st.Flr.SF - First floor square feet

* Bedroom Abv Gr - Number of bedrooms above basement level

* Gr Liv Area - Above grade (ground) living area square feet

* Kitchen Abv Gr - Number of kitchens above grade

* Pool Area - Pool area in square feet

* Lot Area - Lot size in square feet

* BsmtFin.SF.1

* Neighboorhood

* Year Remod Add - Remodel data

* Garage Qual - Garage quality

* MS.SubClass - categorical variable


Due to multicollinearity and low correlation between SalePrice, we eliminated the following variables that we initially believed it would be the potential predictors:

* Tot Rms Abv Grd - Total rooms above grade (does not include bathrooms)

* Garage Cars - Size of garage in car capacity

* Pool Area - Pool area in square feet

* Bsmt.Unf.SF - Basement square feet

* X2nd.Flr.SF - Second floor square feet


After take a look at the scatter plot of the Lot Area, we decided to take a log on Lot.Area.



```{r visualizationLotArea, fig.height = 2.5, fig.width = 6}
# Lot Area
ggplot(train, aes(x = Lot.Area, y = SalePrice)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth(method = "lm") + 
  scale_y_continuous(labels = dollar)
# distribution of lot area
summary(train$Lot.Area)

# ln Lot Area
ggplot(train, aes(x = log(Lot.Area), y = SalePrice)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth(method = "lm") + 
  scale_y_continuous(labels = dollar)

```


\newpage
There seems to be some very high end outliers influencing the scatter plot of sale price and lot area–some lot areas are much larger than most. For this reason, we look at the natural log of lot area instead. We also add a variable to represent the natural log of lot area.


Next, we visualize an important categorical variable Neighborhood because usually neighborhood is an important concern when people choosing apartment.

Th first graph shows the median SalePrice by Neighorhood. The frequency (number of houses) of each Neighborhood in the train set is shown in the labels.

The second graph below shows the frequencies across all data.


```{r visualizationNeighbor, warning=FALSE, fig.height = 4, fig.width = 7}
# using ggplot to visualize the median SalesPrice by Neighboorhood
# Note that the dashed line is median SalePrice
n1 <- ggplot(train[!is.na(train$SalePrice),], aes(x=Neighborhood, y=SalePrice)) +
        geom_bar(stat='summary', fun.y = "median", fill='blue') +
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
        scale_y_continuous(breaks= seq(0, 800000, by=50000), labels = comma) +
        geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3) +
        geom_hline(yintercept=163000, linetype="dashed", color = "red") 

# frenquencies plot across all data
n2 <- ggplot(data=train, aes(x=Neighborhood)) +
        geom_histogram(stat='count')+
        geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
        theme(axis.text.x = element_text(angle = 45, hjust = 1))

# arrange multiple grobs on a page, using 'gridExtra' package
grid.arrange(n1, n2)

```


\newpage
## 5 Data Cleaning

Next, we would like to construct our selected variable in to one subset.
```{r datacleaning}
# using  the subset() function to select out variables from ames huge dataset
train <- subset(train, select=c(SalePrice, Lot.Area, Neighborhood, Garage.Area, X1st.Flr.SF, Kitchen.AbvGr,Gr.Liv.Area, Bedroom.AbvGr, Year.Remod.Add, Year.Built, Overall.Cond, Overall.Qual, Lot.Frontage, BsmtFin.SF.1,MS.SubClass))


# take a look at what we have in the new train variable by using head() function
#head(train)

# is there any missing values in our data?
missingValue <- which(colSums(is.na(train)) > 0)

# sort the missing value column in decreasing order
sort(colSums(sapply(train[missingValue], is.na)), decreasing = TRUE)

# print out numbers of missingValue column in the selected variables dataset
cat('There exist', length(missingValue), 'columns with missing values in the new train dataset.')


```
As we can see there are only 3 column has missingValue, and they are Lot.Frontage, Garage.Qual and BsmtFin.SF.1. We need to fix this.

As NAs mean ‘No Garage’ for character variables, and the missingValue of Garage.Area is a a house built on 1923, we decided to replace it with zero. Also we will replace 0 for BsmtFin.SF.1 as well.
Lot frontage is the linear feet of street connected to property. Although Lot frontage looks like related to neighborhood, bue we do not have any missingValue in neighborhood.

```{r fixmissingValue,warning=FALSE, fig.height = 3, fig.width = 6}

# find the NA of Garage.Area
kable(train[is.na(train$Garage.Area), c('Garage.Area','Year.Built')])

# it was built on 1923, maybe its too old, we will replace it with zero
train$Garage.Area[1785] <- 0

kable(train[is.na(train$BsmtFin.SF.1), c('BsmtFin.SF.1','Year.Built')])
train$BsmtFin.SF.1[1073] <- 0

# summary Lot.Frontage
summary(train$Lot.Frontage)

# okay NA's 402, see which ones are missing lot frontage data.
#index <- which(is.na(train$Lot.Frontage))

# look at the head
#head(train[index,])

# plot Lot.Frontage
ggplot(train, aes(x = Lot.Frontage, y = SalePrice)) + geom_point() + geom_smooth(method = "lm")

# fix MSSubClass categorical variable
train$MS.SubClass <- as.factor(train$MS.SubClass)
 #revalue for better readability
train$MS.SubClass<-revalue(train$MS.SubClass, c('20'='1 story 1946+', '30'='1 story 1945-', '40'='1 story unf attic', '45'='1,5 story unf', '50'='1,5 story fin', '60'='2 story 1946+', '70'='2 story 1945-', '75'='2,5 story all ages', '80'='split/multi level', '85'='split foyer', '90'='duplex all style/age', '120'='1 story PUD 1946+', '150'='1,5 story PUD all', '160'='2 story PUD 1946+', '180'='PUD multilevel', '190'='2 family conversion'))
#str(train$MS.SubClass)
```

## 6 Linear Model

$${SalePrice_t} = \beta_{0} + \beta_{1} LotArea + \beta_{2}  Neighborhood + \beta_{3}  GarageArea + ... + \epsilon$$


```{r fit}
# fit model without MSSubClass categorical variable
fit1 <- lm(SalePrice ~ I(log(Lot.Area)) + Neighborhood + Garage.Area + X1st.Flr.SF+ Kitchen.AbvGr + Gr.Liv.Area + Bedroom.AbvGr + Year.Remod.Add + Year.Built + Overall.Cond + Overall.Qual + Lot.Frontage + BsmtFin.SF.1, data=train)

# fit model with final 14 varibles
fit2 <- lm(SalePrice ~ I(log(Lot.Area)) + Neighborhood + Garage.Area + X1st.Flr.SF+ Kitchen.AbvGr + Gr.Liv.Area + Bedroom.AbvGr + Year.Remod.Add + Year.Built + Overall.Cond + Overall.Qual + Lot.Frontage + BsmtFin.SF.1+ MS.SubClass, data=train)

# checking AIC, pick the model with lower AIC
AIC(fit1, fit2)

# fit2 has lowest AIC, highest R^2, we picked this as final regression model
summary(fit2)
```

**while Neighborhood and MSSubClass catergorical data, it automatically changed to dummy, so this is just two variable.**



## 7 Diagonistic Plots
```{r diagonistic plot, fig1, fig.height = 3, fig.width = 3.5}

plot(fit2)

```

* The first plot indicates the relationship between predictor variables and an outcome variable is approximate linear.
* The QQ-plot looks like we don’t have to be concerned too much since it looks normally distributed.
* The cook's distance plot helps us to find influential cases if any. Not all outliers are influential in linear regression analysis. It looks like none of the outliers in our model are influential.

##Conclusion

Our final multiple linear model contains 14 variables with R^2 0.9031.


**Test the equation**


$$\hat{Y} = -1594000 + 20190x_1 -31050x_2 +19.3x_3 +20.09x_4 -1521x_5 +60.5x_6$$
$$-7292x_7 + 144.2x_8 + 566.5x_9  +5094x_{10} + 13780x_{11} -3644x_{12} +3.103x_{13} + 1460x_{14}$$


$$ \hat{Y} =-1594000+(20190*9.534595)+(-31050)+(19.3*482)+(20.09*928)+(-1521)+(60.5*1629)$$
$$+(-7292*3)+(144.2*1998)+(566.5*1997)+(5094*5)+(13780*5)-(3644*4.304065)+(3.103*791)+1460$$

We did calculate it BY HAND using a calculator, use the training data from the original spreadsheet and manually calculate all transformations andinteractions with our calculator!


Predicted value = $172569.7


Actual value = $189900
